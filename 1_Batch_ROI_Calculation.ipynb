{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Batch Calculation of the Classification Accuracy of ROI Brain Regions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This tutorial demonstrates a complete workflow for decoding fMRI responses associated with free-recall behavior. Using ROI-level activation patterns from multiple cortical regions, we build logistic regression models to predict whether each target image was successfully recalled. The tutorial covers data preprocessing, feature selection using statistical tests, hyperparameter tuning, and cross-validated evaluation. We compare performance using all voxels versus the top statistically informative voxels, and assess significance relative to a shuffled-label baseline. This end-to-end example provides a practical template for fMRI classification analysis and can be easily extended to other cognitive or neuroimaging tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Package Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "from scipy.stats import ttest_ind, t\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, StratifiedKFold\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.impute import SimpleImputer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Parameter Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CSV_PATH = '/BCAI-fmri/free_recall_order.csv'\n",
    "CORT_DIR = '/BCAI-fmri/features/cort'\n",
    "\n",
    "TARGET_IDS = [39, 16, 30, 59, 87, 26]              # The six target picture IDs\n",
    "TOP_N_VOXELS = 10                                   # Number of voxels selected based on t-statistics\n",
    "C_PARAMS = [100, 50, 20, 10, 5, 1, 0.5, 0.2, 0.1,\n",
    "            0.05, 0.01]                              # Regularization strengths for logistic regression\n",
    "RANDOM_STATE = 42\n",
    "CV_FOLDS = 5                                         # 5-fold cross-validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Key Function Define"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def select_top10_voxels(feats, labels):\n",
    "    \"\"\"\n",
    "    Select the top N voxels using two-sample t-test between labels 0 and 1.\n",
    "    The absolute t-value is used for ranking.\n",
    "    \"\"\"\n",
    "    results = []\n",
    "    for voxel_idx in range(feats.shape[1]):\n",
    "        group0 = feats[labels == 0, voxel_idx][~np.isnan(feats[labels == 0, voxel_idx])]\n",
    "        group1 = feats[labels == 1, voxel_idx][~np.isnan(feats[labels == 1, voxel_idx])]\n",
    "        \n",
    "        # Ensure sufficient samples\n",
    "        if len(group0) < 5 or len(group1) < 5:\n",
    "            continue\n",
    "        \n",
    "        t_val, p_val = ttest_ind(group0, group1, equal_var=False, nan_policy='omit')\n",
    "        results.append([voxel_idx, abs(t_val), p_val])\n",
    "\n",
    "    df = pd.DataFrame(results, columns=['voxel_idx', 't_abs', 'p_val']).sort_values('t_abs', ascending=False)\n",
    "    return df.head(TOP_N_VOXELS)['voxel_idx'].tolist()\n",
    "\n",
    "\n",
    "def tune_lr_with_cv(X, y, c_params):\n",
    "    \"\"\"\n",
    "    Tune logistic regression using multiple C values and evaluate using cross-validation.\n",
    "    Returns: mean accuracy, best C, 95% CI, significance (p < 0.05 vs. random baseline).\n",
    "    \"\"\"\n",
    "    # Split data for hyperparameter tuning\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, test_size=0.3, random_state=RANDOM_STATE, stratify=y\n",
    "    )\n",
    "    \n",
    "    scaler = StandardScaler()\n",
    "    X_train_scaled = scaler.fit_transform(X_train)\n",
    "    X_test_scaled = scaler.transform(X_test)\n",
    "    \n",
    "    # Find best C\n",
    "    best_acc = 0\n",
    "    best_c = c_params[0]\n",
    "    \n",
    "    for c in c_params:\n",
    "        lr = LogisticRegression(\n",
    "            C=c, penalty='l2', max_iter=1000, class_weight='balanced',\n",
    "            random_state=RANDOM_STATE\n",
    "        )\n",
    "        lr.fit(X_train_scaled, y_train)\n",
    "        acc = accuracy_score(y_test, lr.predict(X_test_scaled))\n",
    "        \n",
    "        if acc > best_acc:\n",
    "            best_acc = acc\n",
    "            best_c = c\n",
    "\n",
    "    # Cross-validation using the best C\n",
    "    cv = StratifiedKFold(n_splits=CV_FOLDS, shuffle=True, random_state=RANDOM_STATE)\n",
    "    X_scaled = scaler.fit_transform(X)\n",
    "    \n",
    "    cv_scores = cross_val_score(\n",
    "        LogisticRegression(\n",
    "            C=best_c, penalty='l2', max_iter=1000,\n",
    "            class_weight='balanced', random_state=RANDOM_STATE\n",
    "        ),\n",
    "        X_scaled, y, cv=cv, scoring='accuracy'\n",
    "    )\n",
    "    \n",
    "    # Compute 95% Confidence Interval\n",
    "    mean_cv_acc = cv_scores.mean()\n",
    "    sem = np.std(cv_scores) / np.sqrt(len(cv_scores))\n",
    "    ci95 = t.interval(0.95, df=len(cv_scores) - 1, loc=mean_cv_acc, scale=sem)\n",
    "\n",
    "    # Significance test vs. random 0.5 accuracy\n",
    "    t_stat, p_val = ttest_ind(cv_scores, [0.5] * len(cv_scores), alternative='greater')\n",
    "    is_significant = p_val < 0.05\n",
    "    \n",
    "    return mean_cv_acc, best_c, ci95, is_significant\n",
    "\n",
    "\n",
    "def get_random_baseline(X, y, best_c):\n",
    "    \"\"\"\n",
    "    Generate a shuffled-label baseline to evaluate whether the classifier\n",
    "    performs above random chance.\n",
    "    \"\"\"\n",
    "    y_shuffled = np.random.permutation(y)\n",
    "    cv = StratifiedKFold(n_splits=CV_FOLDS, shuffle=True, random_state=RANDOM_STATE)\n",
    "    X_scaled = StandardScaler().fit_transform(X)\n",
    "\n",
    "    random_cv_scores = cross_val_score(\n",
    "        LogisticRegression(\n",
    "            C=best_c, penalty='l2', max_iter=1000,\n",
    "            class_weight='balanced', random_state=RANDOM_STATE\n",
    "        ),\n",
    "        X_scaled, y_shuffled, cv=cv, scoring='accuracy'\n",
    "    )\n",
    "    return random_cv_scores.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Batch Processing Across Brain Regions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_labels = pd.read_csv(CSV_PATH)\n",
    "mask_target = df_labels['pictureID'].isin(TARGET_IDS)\n",
    "labels_all = df_labels.loc[mask_target, 'recall'].values\n",
    "\n",
    "results_list = []\n",
    "npy_files = [f for f in os.listdir(CORT_DIR)\n",
    "             if f.endswith('.npy') and f.startswith('beta_')]\n",
    "\n",
    "print(f\"Starting batch analysis (full voxels + Top {TOP_N_VOXELS} voxels) across {len(npy_files)} brain regions...\")\n",
    "\n",
    "for npy_file in npy_files:\n",
    "    \n",
    "    # Load fMRI features\n",
    "    brain_region_path = os.path.join(CORT_DIR, npy_file)\n",
    "    data = np.load(brain_region_path)\n",
    "    \n",
    "    feats = data[mask_target]\n",
    "    if len(feats) != len(labels_all):\n",
    "        continue\n",
    "    \n",
    "    feats_imputed = SimpleImputer(strategy='mean').fit_transform(feats)\n",
    "\n",
    "    # ------------------ Full Voxel Logistic Regression ------------------\n",
    "    mean_acc_full, best_c_full, ci95_full, is_significant_full = tune_lr_with_cv(\n",
    "        feats_imputed, labels_all, C_PARAMS\n",
    "    )\n",
    "    random_acc_full = get_random_baseline(feats_imputed, labels_all, best_c_full)\n",
    "\n",
    "    # ------------------ Top 10 Voxel Logistic Regression ------------------\n",
    "    top_voxels = select_top10_voxels(feats_imputed, labels_all)\n",
    "    \n",
    "    if len(top_voxels) < TOP_N_VOXELS:\n",
    "        mean_acc_top = np.nan\n",
    "        best_c_top = np.nan\n",
    "        ci95_top = (np.nan, np.nan)\n",
    "        is_significant_top = 'N/A'\n",
    "        random_acc_top = np.nan\n",
    "\n",
    "    else:\n",
    "        X_top = feats_imputed[:, top_voxels]\n",
    "        mean_acc_top, best_c_top, ci95_top, is_significant_top = tune_lr_with_cv(\n",
    "            X_top, labels_all, C_PARAMS\n",
    "        )\n",
    "        random_acc_top = get_random_baseline(X_top, labels_all, best_c_top)\n",
    "\n",
    "    # Collect results\n",
    "    brain_region_name = npy_file.replace('beta_', '').replace('.npy', '')\n",
    "    \n",
    "    results_list.append({\n",
    "        'Region': brain_region_name,\n",
    "\n",
    "        # Full voxel results\n",
    "        'FullVoxels-BestC': best_c_full,\n",
    "        'FullVoxels-Accuracy': round(mean_acc_full, 3),\n",
    "        'FullVoxels-RandomAcc': round(random_acc_full, 3),\n",
    "        'FullVoxels-95CI': f\"[{round(ci95_full[0], 3)}, {round(ci95_full[1], 3)}]\",\n",
    "        'FullVoxels-Significant': 'Yes' if is_significant_full else 'No',\n",
    "\n",
    "        # Top voxel results\n",
    "        'TopVoxels-BestC': best_c_top if not np.isnan(best_c_top) else 'N/A',\n",
    "        'TopVoxels-Accuracy': round(mean_acc_top, 3) if not np.isnan(mean_acc_top) else 'N/A',\n",
    "        'TopVoxels-RandomAcc': round(random_acc_top, 3) if not np.isnan(random_acc_top) else 'N/A',\n",
    "        'TopVoxels-95CI': f\"[{round(ci95_top[0], 3)}, {round(ci95_top[1], 3)}]\"\n",
    "                          if not np.isnan(ci95_top[0]) else 'N/A',\n",
    "        'TopVoxels-Significant': (\n",
    "            'Yes' if is_significant_top is True else\n",
    "            'No' if is_significant_top is False else 'N/A'\n",
    "        ),\n",
    "    })"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Output Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df = pd.DataFrame(results_list).sort_values(\n",
    "    'TopVoxels-Accuracy', ascending=False, na_position='last'\n",
    ")\n",
    "\n",
    "print(\"\\n\" + \"=\"*180)\n",
    "print(\"Batch Analysis Results Across Brain Regions (Full Voxels vs. Top Voxels)\")\n",
    "print(\"=\"*180)\n",
    "print(results_df.to_string(index=False))\n",
    "\n",
    "print(\"\\n\" + \"=\"*180)\n",
    "print(f\"Top {TOP_N_VOXELS} Voxels â€” Top 10 Regions by Accuracy\")\n",
    "print(\"=\"*180)\n",
    "top10_brains = results_df[results_df['TopVoxels-Accuracy'] != 'N/A'].head(10)\n",
    "print(top10_brains.to_string(index=False))\n",
    "print(\"=\"*180)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fmri",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.23"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
